{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost\n",
    "import sklearn\n",
    "# import seaborn as sns\n",
    "%matplotlib inline\n",
    "data_path = '/Users/dirlt/.kaggle/competitions/bike-sharing-demand/'\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics\n",
    "from sklearn.base import BaseEstimator, RegressorMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "links:\n",
    "- https://www.kaggle.com/miteshyadav/comprehensive-eda-with-xgboost-top-10-percentile/notebook\n",
    "- https://www.kaggle.com/viveksrinivasan/eda-ensemble-model-top-10-percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mytrain.csv')\n",
    "test_df = pd.read_csv('mytest.csv')\n",
    "X, y = df.drop(['casual', 'registered', 'count'], axis = 1), np.log1p(df[['casual', 'registered', 'count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(x, casual = True):\n",
    "    drop_fields = ['dt_day', 'dt_hour', 'season', 'weather', 'dt_year', 'dt_month', 'dt_weekday', 'atemp']\n",
    "    if 'datetime' in x.columns:\n",
    "        drop_fields.append('datetime')\n",
    "    return x.drop(drop_fields, axis = 1)\n",
    "\n",
    "def make_cv(X,n = 2):\n",
    "    for i in range(n):\n",
    "        days = [x + i for x in [18-i, 19-i]]\n",
    "        train_idx = X[X['dt_day'].apply(lambda x: x not in days)].index\n",
    "        test_idx = X[X['dt_day'].apply(lambda x: x in days)].index\n",
    "        yield train_idx, test_idx\n",
    "\n",
    "def rmse(x, y):\n",
    "    return mean_squared_error(x, y) ** 0.5\n",
    "\n",
    "def print_features(names, values, thres = 0.01):\n",
    "    fts = list(zip(names, values))\n",
    "    fts.sort(key = lambda x: -x[1])\n",
    "    ns = []\n",
    "    for idx, (name, value) in enumerate(fts):\n",
    "        if value < thres: break\n",
    "        print('- {} {:.2f}'.format(name, value))\n",
    "        ns.append(name)\n",
    "    print(format(','.join(ns)))\n",
    "\n",
    "class MyEstimator(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        if 'ma' in kwargs:\n",
    "            self.ma = kwargs['ma']\n",
    "            del kwargs['ma']\n",
    "        if 'mb' in kwargs:\n",
    "            self.mb = kwargs['mb']\n",
    "            del kwargs['mb']\n",
    "        kwargs['init'] = True\n",
    "        self.set_params(**kwargs)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        input_a = input_fn(X, casual=True)\n",
    "        input_b = input_fn(X, casual=False)\n",
    "        self.ma.fit(input_a, y['casual'])\n",
    "        self.mb.fit(input_b, y['registered'])\n",
    "        self.ca = input_a.columns\n",
    "        self.cb = input_b.columns\n",
    "        \n",
    "    def predict(self, X):\n",
    "        ya = self.ma.predict(input_fn(X, casual=True))\n",
    "        yb = self.mb.predict(input_fn(X, casual=False))\n",
    "        y = np.log1p(np.expm1(ya) + np.expm1(yb))\n",
    "        return y\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y2 = self.predict(X)\n",
    "        return -rmse(y['count'], y2)\n",
    "        \n",
    "    def set_params(self, **params):\n",
    "        pa = {}\n",
    "        pb = {}\n",
    "        for k in params:\n",
    "            if k.startswith('a_'):\n",
    "                pa[k[2:]] = params[k]\n",
    "            elif k.startswith('b_'):\n",
    "                pb[k[2:]] = params[k]\n",
    "            else:\n",
    "                pass\n",
    "        if 'init' not in params:\n",
    "            #print(pa, pb)\n",
    "            pass\n",
    "        self.ma.set_params(**pa)\n",
    "        self.mb.set_params(**pb)\n",
    "        return self\n",
    "        \n",
    "    def get_params(self, deep = True):\n",
    "        pa = self.ma.get_params(deep)\n",
    "        pb = self.ma.get_params(deep)\n",
    "        p = {}\n",
    "        for k in pa:\n",
    "            p['a_' + k] = pa[k]\n",
    "        for k in pb:\n",
    "            p['b_' + k] = pb[k]\n",
    "        p['ma'] = self.ma\n",
    "        p['mb'] = self.mb\n",
    "        return p\n",
    "    \n",
    "    def print_features(self, thres = 0.005):\n",
    "        ca = self.ca\n",
    "        cb = self.cb\n",
    "        print('=====casual=====')\n",
    "        print_features(ca, self.ma.feature_importances_, thres)\n",
    "        print('=====registered=====')\n",
    "        print_features(cb, self.mb.feature_importances_, thres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv for rf model\n",
      "Fitting 2 folds for each of 25 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:  8.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3450205441974117 {'a_min_samples_split': 9, 'b_min_samples_split': 6}\n",
      "CPU times: user 32min 54s, sys: 21.8 s, total: 33min 15s\n",
      "Wall time: 9min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('cv for rf model')\n",
    "rf0 = RandomForestRegressor(n_estimators=400, random_state = 42, verbose=0, n_jobs=4)\n",
    "rf1 = RandomForestRegressor(n_estimators=400, random_state = 42, verbose=0, n_jobs=4)\n",
    "rf = MyEstimator(ma = rf0, mb = rf1)\n",
    "params = {'a_min_samples_split': [8,9,10,11,12], 'b_min_samples_split': [4,5,6,7,8]}\n",
    "rf_cv = GridSearchCV(rf, params, cv = make_cv(X,2), n_jobs = 1, verbose = 1)\n",
    "rf_cv.fit(X, y)\n",
    "print(rf_cv.best_score_, rf_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 33s, sys: 2.41 s, total: 3min 35s\n",
      "Wall time: 56.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_best_params = rf_cv.best_params_.copy()\n",
    "# rf_best_params = {'a_min_samples_split': 8, 'b_min_samples_split': 5}\n",
    "rf_best_params['a_n_estimators'] = 2000\n",
    "rf_best_params['b_n_estimators'] = 2000\n",
    "rf.set_params(**rf_best_params)\n",
    "rf.fit(X, y)\n",
    "output_y = rf.predict(test_df)\n",
    "output = np.round(np.expm1(output_y)).astype(int)\n",
    "output[output < 0] = 0\n",
    "df_output = pd.DataFrame({'datetime': test_df['datetime'], 'count': output}, columns=('datetime', 'count'))\n",
    "df_output['count'] = df_output['count'].astype(int)\n",
    "df_output.to_csv('submission-rf.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv for gbm model\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  48 out of  48 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3135079249588759 {'a_max_depth': 4, 'b_max_depth': 6}\n",
      "CPU times: user 9.59 s, sys: 171 ms, total: 9.76 s\n",
      "Wall time: 3min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('cv for gbm model')\n",
    "gbm0 = GradientBoostingRegressor(n_estimators=200, random_state = 42, verbose=0)\n",
    "gbm1 = GradientBoostingRegressor(n_estimators=200, random_state = 42, verbose=0)\n",
    "gbm = MyEstimator(ma = gbm0, mb = gbm1)\n",
    "params = {'a_max_depth': [3,4,5,6,7,8], 'b_max_depth': [5,6,7,8]}\n",
    "gbm_cv = GridSearchCV(gbm, params, cv = make_cv(X,2), n_jobs = 4, verbose = 1)\n",
    "gbm_cv.fit(X, y)\n",
    "print(gbm_cv.best_score_, gbm_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.3 s, sys: 112 ms, total: 45.4 s\n",
      "Wall time: 45.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gbm_best_params = gbm_cv.best_params_.copy()\n",
    "# gbm_best_params = {'a_min_samples_split': 5, 'b_min_samples_split': 3}\n",
    "# gbm_best_params = {'a_max_depth': 6, 'b_max_depth':6}\n",
    "gbm_best_params['a_n_estimators'] = 1000\n",
    "gbm_best_params['b_n_estimators'] = 1000\n",
    "gbm.set_params(**gbm_best_params)\n",
    "gbm.fit(X, y)\n",
    "output_y = gbm.predict(test_df)\n",
    "output = np.round(np.expm1(output_y)).astype(int)\n",
    "output[output < 0] = 0\n",
    "df_output = pd.DataFrame({'datetime': test_df['datetime'], 'count': output}, columns=('datetime', 'count'))\n",
    "df_output['count'] = df_output['count'].astype(int)\n",
    "df_output.to_csv('submission-gbm.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv for xgb model\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3114569346253781 {'a_max_depth': 4, 'b_max_depth': 6}\n",
      "CPU times: user 6min 32s, sys: 39.3 s, total: 7min 11s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('cv for xgb model')\n",
    "xgb0 = XGBRegressor(n_estimators=200, random_state = 42, verbose=0, n_jobs=4)\n",
    "xgb1 = XGBRegressor(n_estimators=200, random_state = 42, verbose=0, n_jobs=4)\n",
    "xgb = MyEstimator(ma = xgb0, mb = xgb1)\n",
    "params = {'a_max_depth': [3,4,5,6,7,8], 'b_max_depth': [5,6,7,8]}\n",
    "xgb_cv = GridSearchCV(xgb, params, cv = make_cv(X,2), n_jobs = 1, verbose = 1)\n",
    "xgb_cv.fit(X, y)\n",
    "print(xgb_cv.best_score_, xgb_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.9 s, sys: 2.61 s, total: 35.5 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_best_params = xgb_cv.best_params_.copy()\n",
    "#xgb_best_params = {'a_max_depth': 7, 'b_max_depth':7}\n",
    "xgb_best_params['a_n_estimators'] = 1000\n",
    "xgb_best_params['b_n_estimators'] = 1000\n",
    "xgb.set_params(**xgb_best_params)\n",
    "xgb.fit(X, y)\n",
    "output_y = xgb.predict(test_df)\n",
    "output = np.round(np.expm1(output_y)).astype(int)\n",
    "output[output < 0] = 0\n",
    "df_output = pd.DataFrame({'datetime': test_df['datetime'], 'count': output}, columns=('datetime', 'count'))\n",
    "df_output['count'] = df_output['count'].astype(int)\n",
    "df_output.to_csv('submission-xgb.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf = pd.read_csv('submission-rf.csv')\n",
    "df_gbm = pd.read_csv('submission-gbm.csv')\n",
    "df_xgb = pd.read_csv('submission-xgb.csv')\n",
    "df_avg = pd.DataFrame(df_rf)\n",
    "df_avg['count'] = np.round((df_rf['count'] + df_gbm['count'] + df_xgb['count'] + 1) * 0.33).astype(int)\n",
    "# df_avg['count'] = np.round((df_rf['count'] + df_gbm['count'] + 1) * 0.5).astype(int)\n",
    "df_avg.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to Bike Sharing Demand"
     ]
    }
   ],
   "source": [
    "message = 'no outlier, add windspeed_0, grid search cv with low n_estimators, but run with high n_estimators'\n",
    "!kaggle competitions submit -c bike-sharing-demand -f submission.csv -m \"$message\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[submission link](https://www.kaggle.com/c/bike-sharing-demand/submissions?sortBy=date&group=all&page=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
